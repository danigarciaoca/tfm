#LyX 2.2 created this file. For more info see http://www.lyx.org/
\lyxformat 508
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass book
\begin_preamble


\AtBeginDocument{
  \def\labelitemii{\ding{71}}
  \def\labelitemiii{\ding{111}}
  \def\labelitemiv{\(\vartriangleright\)}
}



\usepackage{babel}
\addto\shorthandsspanish{\spanishdeactivate{~<>}}
\end_preamble
\use_default_options false
\maintain_unincluded_children false
\language spanish
\language_package default
\inputencoding iso8859-15
\fontencoding T1
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing double
\use_hyperref true
\pdf_bookmarks false
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks false
\pdf_pdfborder false
\pdf_colorlinks false
\pdf_backref section
\pdf_pdfusetitle false
\papersize a4paper
\use_geometry true
\use_package amsmath 2
\use_package amssymb 2
\use_package cancel 0
\use_package esint 1
\use_package mathdots 0
\use_package mathtools 0
\use_package mhchem 0
\use_package stackrel 0
\use_package stmaryrd 0
\use_package undertilde 0
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 1.5in
\topmargin 1.6in
\rightmargin 1.2in
\bottommargin 1.6in
\headheight 1.5in
\headsep 0.3in
\footskip 0.8in
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language french
\papercolumns 1
\papersides 1
\paperpagestyle plain
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset FormulaMacro
\renewcommand{\T}{\mathcal{\top}}
\end_inset


\begin_inset FormulaMacro
\renewcommand{\S}{\mathcal{S}}
\end_inset


\begin_inset FormulaMacro
\renewcommand{\A}{\mathcal{A}}
\end_inset


\begin_inset FormulaMacro
\renewcommand{\R}{\mathcal{R}}
\end_inset


\begin_inset FormulaMacro
\renewcommand{\P}{\mathcal{P}}
\end_inset


\begin_inset FormulaMacro
\renewcommand{\Pr}{\mathbb{P}}
\end_inset


\begin_inset FormulaMacro
\renewcommand{\Sa}{\mathbb{S}}
\end_inset


\begin_inset FormulaMacro
\renewcommand{\Ba}{\mathbb{B}}
\end_inset


\begin_inset FormulaMacro
\renewcommand{\Na}{\mathbb{N}}
\end_inset


\end_layout

\begin_layout Chapter
Estado del arte
\end_layout

\begin_layout Standard
El aprendizaje por refuerzo es una rama del aprendizaje máquina o 
\emph on
machine learning
\emph default
 en la que se lleva trabajando desde hace años debido al interés que suscita
 su aplicación en situaciones cotidianas de la vida real.
 No obstante, en los últimos años ha ganado más fuerza como consecuencia
 de los avances que se han producido en cuanto a capacidad computacional
 y a que el mercado está demandado soluciones que utilicen esta tecnología
 (ej.: robots que puedan atender las necesidades de los clientes o coches
 autónomos capaces de conducir solos).
 En consecuencia, se ha puesto mayor interés en desarrollar algoritmos sólidos
 que permitan resolver problemas de esta naturaleza, y han surgido una serie
 de métodos que se han convertido en los más empleados a día de hoy.
 A grandes rasgos, los algoritmos existentes se pueden dividir en dos grupos
 según la dimensión que tenga el problema a tratar:
\end_layout

\begin_layout Enumerate
Orientados a problemas con un conjunto pequeño y discreto de estados: cuando
 se tienen estas condiciones, la dimensionalidad es aún razonable y se puede
 disponer de una tabla que almacene las funciones valor de cada estado del
 conjunto de estados 
\begin_inset Formula $\S$
\end_inset

 y la política obtenida.
 Los algoritmos más extendidos para resolver este tipo de problemas de RL
 son SARSA, Q-learning y doble Q-learning.
 Por supuesto, existen variantes de estos tres algoritmos que introducen
 ligeras mejoras orientadas a la reducción de varianza en la estimación
 o a la corrección del sesgo.
 No obstante, su comportamiento es similar al de sus versiones primigenias.
\end_layout

\begin_layout Enumerate
Orientados a problemas con un conjunto grande o continuo de estados: cuando
 el problema adquiere una dimensionalidad mayor, deja de ser factible la
 idea de almacenar las funciones valor y la política en una tabla, pues
 consumirían toda la memoria disponible; en consecuencia, los algoritmos
 anteriores dejan de tener validez.
 Es por ello que se hace necesario emplear aproximaciones que generalicen
 la función valor para subconjuntos de estados cercanos entre sí, y permitan
 por tanto reducir la dimensionalidad del problema.
 En función del tipo de aproximación que se emplee, podremos encontrar las
 siguientes soluciones:
\end_layout

\begin_deeper
\begin_layout Enumerate
Aproximación lineal: los algoritmos más empleados cuando se utilizan aproximacio
nes lineales de la función valor son 
\emph on
Least-Squares Policy Iteration
\emph default
 (LSPI), 
\emph on
Gradient Temporal Difference
\emph default
 (GTD) y GTD versión 2 (GTD2).
\end_layout

\begin_layout Enumerate
Aproximación no lineal mediante redes neuronales: de reciente aparición
 debido al resurgimiento de las redes neuronales, los algoritmos existentes
 cuando se emplea este tipo de aproximaciones son Deep Q-networks (DQN)
 y Neural Fitted Q-iteration (NFQ).
\end_layout

\end_deeper
\begin_layout Standard
Todos los algoritmos anteriores entran dentro de la categoría de los llamados
 métodos basados en la función valor
\emph on
 
\emph default
(
\emph on
value-based
\emph default
 
\emph on
methods
\emph default
).
 En lo que respecta a los algoritmos basados en el gradiente de la política,
 aún no existe mucha literatura escrita más allá de los métodos actor-crítico,
 los cuales únicamente se emplean cuando se usa aproximación de funciones,
 y algunas publicaciones presentando ideas teóricas al respecto 
\begin_inset CommandInset citation
LatexCommand cite
key "Wang2008"

\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "Chen2016"

\end_inset

.
 Por tanto, queda aún mucho terreno que explotar en lo que a los métodos
 basados en el gradiente de la política se refiere, y será lo que se pretenda
 conseguir con este trabajo.
\end_layout

\begin_layout Standard
Dado que por limitaciones de tiempo resultará imposible contrastar el algoritmo
 que se desarrolle con todos los citados anteriormente, los resultados obtenidos
 se enfrentarán únicamente a aquellos que se han considerado más representativos
 y que son más empleados a día de hoy: SARSA, Q-learning y LSPI.
\end_layout

\end_body
\end_document
