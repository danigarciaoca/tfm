#LyX 2.2 created this file. For more info see http://www.lyx.org/
\lyxformat 508
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass book
\begin_preamble
\usepackage{optidef}
\usepackage[bottom]{footmisc}
\input{spanishPseudoAlgorithmic} % mi archivo de traducción
\renewcommand\spanishtablename{Tabla}

\usepackage{mathtools}

\AtBeginDocument{
  \def\labelitemii{\ding{71}}
  \def\labelitemiii{\ding{111}}
  \def\labelitemiv{\(\vartriangleright\)}
}


\usepackage{babel}
\addto\shorthandsspanish{\spanishdeactivate{~<>}}
\end_preamble
\use_default_options false
\begin_modules
theorems-ams-bytype
theorems-chap-bytype
theorems-ams-extended-bytype
\end_modules
\maintain_unincluded_children false
\language spanish
\language_package default
\inputencoding iso8859-15
\fontencoding T1
\font_roman "lmodern" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing double
\use_hyperref true
\pdf_bookmarks false
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks false
\pdf_pdfborder false
\pdf_colorlinks false
\pdf_backref section
\pdf_pdfusetitle false
\papersize a4paper
\use_geometry true
\use_package amsmath 1
\use_package amssymb 2
\use_package cancel 0
\use_package esint 1
\use_package mathdots 0
\use_package mathtools 0
\use_package mhchem 0
\use_package stackrel 0
\use_package stmaryrd 0
\use_package undertilde 0
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 1.5in
\topmargin 1.6in
\rightmargin 1.2in
\bottommargin 1.6in
\headheight 1.5in
\headsep 0.3in
\footskip 0.8in
\secnumdepth 2
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language french
\papercolumns 1
\papersides 1
\paperpagestyle plain
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset FormulaMacro
\renewcommand{\T}{\mathcal{\top}}
\end_inset


\begin_inset FormulaMacro
\renewcommand{\S}{\mathcal{S}}
\end_inset


\begin_inset FormulaMacro
\renewcommand{\A}{\mathcal{A}}
\end_inset


\begin_inset FormulaMacro
\renewcommand{\R}{\mathcal{R}}
\end_inset


\begin_inset FormulaMacro
\renewcommand{\P}{\mathcal{P}}
\end_inset


\begin_inset FormulaMacro
\renewcommand{\Pr}{\mathbb{P}}
\end_inset


\begin_inset FormulaMacro
\renewcommand{\Sa}{\mathbb{S}}
\end_inset


\begin_inset FormulaMacro
\renewcommand{\Ba}{\mathbb{B}}
\end_inset


\begin_inset FormulaMacro
\renewcommand{\Na}{\mathbb{N}}
\end_inset


\end_layout

\begin_layout Chapter
Conclusiones y trabajo futuro
\end_layout

\begin_layout Section
Conclusiones
\end_layout

\begin_layout Standard
En este trabajo se ha presentado el desarrollo de un algoritmo para la resolució
n del problema de aprendizaje por refuerzo, basado en un punto de vista
 alternativo y poco estudiado hasta el momento en la literatura: la exploración
 del espacio de políticas.
\end_layout

\begin_layout Standard
Para alcanzar este nuevo enfoque, se ha partido del problema de control
 óptimo y su formulación como un programa lineal, hasta llegar al problema
 dual asociado.
 A raíz del problema dual, se han hecho una serie de interpretaciones derivadas
 de la formulación del problema de aprendizaje por refuerzo como un MDP,
 que han permitido establecer un vínculo entre la variable dual del problema
 dual y la política de comportamiento del problema de control.
 De este modo, se ha llegado a la conclusión de que encontrar la solución
 óptima del problema dual equivale a encontrar la política de comportamiento
 óptima, y por tanto a resolver el problema de RL.
 Tras darle esta interpretación a la variable dual, se ha presentado una
 manera de resolver el problema dual basada en el conocido método de ascenso
 dual, al cual se le han hecho una serie de modificaciones dando lugar al
 método Bellman-ascenso dual.
 
\end_layout

\begin_layout Standard
Mediante la elección de este método basado en gradiente, se pudo formular
 un algoritmo que obtiene una estimación de dicho gradiente de manera estocástic
a a partir de las muestras de experiencia generadas al interactuar con el
 entorno.
 Como resultado, se derivó el algoritmo de aprendizaje por refuerzo BDA-MF.
\end_layout

\begin_layout Standard
Con intención de comparar el desempeño de BDA-MF con el estado del arte,
 se resolvieron una serie de problemas típicos y se compararon los resultados
 obtenidos con SARSA y Q-learning.
 Tras estas pruebas, pudo observarse que el nuevo algoritmo desarrollado
 se comportaba mejor que los ya existentes en situaciones en que el problema
 presentaba cierto carácter aleatorio.
 A raíz de ello, se llegó a la conclusión de que BDA-MF es más sensible
 a la exploración que SARSA y Q-learning.
\end_layout

\begin_layout Standard
Una vez formalizado y probado el algoritmo novel con problemas de pequeña
 escala, se extendieron todas estas ideas al caso en que el conjunto de
 estados es grande o continuo, dando lugar al algoritmo BDALA-MF.
 Este algoritmo basa su funcionamiento en las mismas ideas que el anterior,
 pero ahora se lleva a cabo la estimación de la función valor y de la variable
 dual a través de aproximaciones lineales.
\end_layout

\begin_layout Standard
Finalmente, se comparó el comportamiento de BDALA-MF con el estado del arte
 mediante la resolución de dos problemas típicos, y se llegó a la conclusión
 de que la aproximación de la variable dual degrada considerablemente el
 comportamiento del algoritmo desarrollado.
\end_layout

\begin_layout Section
Trabajo futuro
\end_layout

\begin_layout Standard
Como líneas de trabajo futuro, se plantean una serie de mejoras de los algoritmo
s presentados en este documento.
\end_layout

\begin_layout Standard
La primera de ellas consiste en resolver el problema de la linealidad del
 Lagrangiano, detallado en la sección 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Solución-prob-dual"

\end_inset

.
 Para ello se propone tomar el Lagrangiano aumentado, el cual introduce
 un término cuadrático que podría además mejorar las propiedades de convergencia
 del algoritmo.
 Al solucionar el problema de la linealidad, se estudiaría la posibilidad
 de utilizar el método Arrow-Hurwicz como alternativa a la técnica Bellman-ascen
so dual empleada.
\end_layout

\begin_layout Standard
La segunda mejora que se propone, relativa a la etapa de predicción de BDA-MF,
 consiste en sustituir la resolución de la ecuación de Bellman mediante
 TD por una versión similar pero que incorpora algunas mejoras: TD
\begin_inset Formula $\left(\lambda\right)$
\end_inset

.
 Aunque este método no ha sido estudiado en este trabajo, a grandes rasgos
 consiste en una versión intermedia entre la estimación por TD y Monte-Carlo,
 de manera que se consigue mantener una varianza reducida, como ocurre con
 TD, a la par que se lleva a cabo la reducción del sesgo característica
 del método de Monte-Carlo.
\end_layout

\begin_layout Standard
Una tercera consideración de cara a aumentar el rendimiento de los algoritmos
 desarrollados, será la mejora en la aproximación del gradiente.
 En lugar de emplear una estimación basada en la muestra del instante actual,
 se propone hacer uso de métodos más sofisticados como AdaGrad, RMSProp
 o Adam.
 Con ello se pretende acelerar la convergencia de los gradientes, y por
 tanto la búsqueda de la política óptima.
\end_layout

\begin_layout Standard
La última, y quizás más importante, de las mejoras que se proponen, es la
 extensión del método Bellman-ascenso dual a problemas de gran escala mediante
 el uso de aproximaciones de funciones no lineales, más concretamente a
 través de redes neuronales.
 La combinación de la teoría de aprendizaje por refuerzo con los conceptos
 relativos a redes neuronales es lo que a día de hoy se conoce como aprendizaje
 por refuerzo profundo (
\emph on
Deep Reinforcement Learning
\emph default
, DPL), y está dando lugar a nuevos algoritmos muy potentes, empleados por
 grandes empresas como Google, Facebook o Microsoft.
 Por ello, combinar las ideas del algoritmo aquí planteado con la aproximación
 mediante redes neuronales, podría suponer un gran avance en el aprendizaje
 por refuerzo profundo a través de la exploración del espacio de políticas.
\end_layout

\end_body
\end_document
